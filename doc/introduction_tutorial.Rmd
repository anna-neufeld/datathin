---
title: "Introductory tutorial" 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introductory tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE
)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

Welcome to the `datathin` package. In this tutorial, we will show how random variables from different distributions can be split into independent training and test components. For more details, see our pre-print. For information on how to use `datathin` for tasks such as model evaluation or inference after model selection, see our forthcoming tutorials. 

To get started, ensure that you have downloaded the package. Make sure that ``remotes`` is installed by running ``install.packages("remotes")``, then type

```{r,eval=FALSE}
remotes::install_github("anna-neufeld/datathin")
```

```{r}
library(datathin)
```


# Poisson

The Poisson distribution is simple, because there is only one parameter. 

We first generate a vector of 10,000 $Poisson(7)$ random variables. 

```{r}
set.seed(1)
dat <- rpois(10000, 7)
```

```{r}
dat.thin <- datathin(dat, family="poisson", epsilon=0.3)
dat.train <- dat.thin$Xtr
dat.test <- dat.thin$Xte
```

We now verify several properties of the data thinning operation.

First, we verify that the expected value of dat.train is $0.3 \times 7$ and the expected value of dat.test is $0.7 \times 7$. 

```{r, hold=TRUE}
mean(dat)
mean(dat.train)
0.3*7
mean(dat.test)
0.7*7
```

We next verify that `dat.train` and `dat.test` are independent, and that they sum to `dat`. Throughout this document, we use small empirical correlations as evidence of independence. 

```{r}
all.equal(dat, as.numeric(dat.train+dat.test))
cor(dat.train, dat.test)
```


# Exponential

We next consider the exponential distribution. 

```{r}
set.seed(2)
dat <- rexp(100000, rate=1/3)
mean(dat)
```

For this distribution, we demonstrate how to create multiple indendent folds of data using the ``multithin`` function. 

```{r}
folds <- multithin(dat, family="exponential", nfolds=5)
```

Here, `folds` is a list with 5 elements. Each list element stores one fold of data, which in this case means a vector of length 100000.

```{r}
length(folds)
length(folds[[1]])
```

We can verify that, for $m=1,\ldots,5$, the $m$th fold of data is independent of the sum of the remaining folds, and that it has mean $\frac{1}{5} \times 3$, while the remainder has mean $\frac{4}{5} \times 3$. Once again, we use small empirical correlations as evidence of independence.  

```{r}
for (m in 1:5) {
  print("-------")
  print(paste("fold", m))
  
  dat.test <- folds[[m]]
  dat.train <- dat - folds[[m]]
  
  print(c(1/5*mean(dat), mean(dat.test)))
  print(c(4/5*mean(dat), mean(dat.train)))

  print(as.numeric(cor(dat.test, dat.train)))
}
```


# Normal distribution

We now show an example of thinning the normal distribution. This is slightly more complicated, as the $N(\mu, \sigma^2)$ distribution has two parameters, and in order to apply data thinning the parameter $\sigma^2$ must be known. 

We start by generating data from a $N(5, 2)$ distribution and applying ``datathin`` with $\epsilon = 0.5$. This time, we let our dataset `dat` be a matrix with dimensions $10000 \times 10$. 

```{r}
set.seed(3)
dat <- matrix(rnorm(10000*10, mean=5, sd=sqrt(2)), nrow=10000)
```

We first apply data thinning assuming that $\sigma^2=2$ is known. This is passed in as the ``arg`` parameter. We can see that, after applying data thinning, each column of the training set is independent of each column in the test set (once again, we are using small empirical correlations as evidence of independence).

```{r}
res <- datathin(dat, family="normal", epsilon=0.5, arg=2)
dat.train <- res$Xtr
dat.test <- res$Xte

sapply(1:ncol(dat.train), function(u) cor(dat.train[,u], dat.test[,u]))
```

The parameter `arg` must either be a scalar (in which case it is assumed that all elements of the data matrix have the same value of $\sigma^2$), or its dimensions must match that of the data.

We now explore the impact of using the "wrong" value of $\sigma^2$ while splitting. We generate a dataset with $100,000$ rows and $3$ columns. In the first column, the data are from a $N(5, 0.1)$ distribution. In the second column, they are from a $N(5,2)$ distribution. In the final column, they are from a $N(5,20)$ distribution. 

```{r}
dat <- cbind(rnorm(100000, mean=5, sd=sqrt(0.1)),
             rnorm(100000, mean=5, sd=sqrt(2)),
             rnorm(100000, mean=5, sd=sqrt(20)))
```

First, we datathin but we incorrectly that all items in the matrix have $\sigma^2=2$. We see that we have negative correlation between the training and test sets in the first column (where we used a value of $\sigma^2$ that was too big), no correlation in the second column (where we used the correct value of $\sigma^2$), and positive correlation in the last column (where we used a value of $\sigma^2$ that was too small). See our preprint for further explanation of this property. 

```{r}
res <- datathin(dat, family="normal", epsilon=0.5, arg=2)
dat.train <- res$Xtr
dat.test <- res$Xte
sapply(1:ncol(dat.train), function(u) cor(dat.train[,u], dat.test[,u]))
```

To remedy the situation, we must let `arg` be a matrix that stores the correct $\sigma^2$ values.

```{r}
good_args <- cbind(rep(0.1, 100000), rep(2, 100000), rep(20,100000))
res <- datathin(dat, family="normal", epsilon=0.5, arg=good_args)
dat.train <- res$Xtr
dat.test <- res$Xte
sapply(1:ncol(dat.train), function(u) cor(dat.train[,u], dat.test[,u]))
```

We quickly note that an error will be thrown if `arg` is not a scalar and its dimensions do not match those of the data. The following code results in an error:

```{r, eval=F}
res <- datathin(dat, family="normal", epsilon=0.5, arg=c(0.1,2,20))
```

# Negative binomial

We generate data from a negative binomial distribution. We use the mean and overdispersion parameterization of the negative binomial distribution. 

```{r}
dat <- rnbinom(100000, size=7, mu = 6)
res <- datathin(dat, family="negative binomial", epsilon = 0.2, arg=7)
dat.train <- res$Xtr
dat.test <- res$Xte
0.2*6
mean(dat.train)
0.8*6
mean(dat.test)
as.numeric(cor(dat.train, dat.test))
```

# Gamma 

We verify the same properties as above, assuming that the shape parameter is known. 

```{r}
dat <- rgamma(10000, shape=12, rate=2)
mean(dat)
res <- datathin(dat, family="gamma", epsilon = 0.5, arg=12)
dat.train <- res$Xtr
dat.test <- res$Xte
mean(dat.train)
mean(dat.test)
as.numeric(cor(dat.train, dat.test))
```


# Binomial 

We verify the same properties as above, assuming that the size parameter is known. Note that, if we want to use $\epsilon=0.5$, the size parameter of the binomial distribution must be an even number. 

```{r}
dat <- rbinom(10000, 16, 0.25)
mean(dat)
res <- datathin(dat, family="binomial", epsilon = 0.5, arg=16)
dat.train <- res$Xtr
dat.test <- res$Xte
mean(dat.train)
mean(dat.test)
as.numeric(cor(dat.train, dat.test))
```


# Multivariate normal and multinomial.

Implementations and demonstrations for these multivariate distributions are coming soon! 
