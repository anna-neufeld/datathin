---
title: "Introductory tutorial" 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introductory tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE
)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

Welcome to the `datathin` package. In this tutorial, we will show how random variables from different distributions can be split into independent training at test components. For more details, see our preprint. For information on how to use `datathin` for tasks such as model evalation or inference after model selection, see our forthcoming tutorials. 

To get started, ensure that you have downloaded and loaded the package:

```{r,eval=FALSE}
remotes::install_github("anna-neufeld/datathin")
```

```{r}
library(datathin)
```


# Poisson 

We start by considering Poisson random variables. This is a simple case, as the Poisson distribution has only one parameter.

We first generate a vector of 10,000 $Poisson(7)$ random variables. 

```{r}
set.seed(1)
dat <- rpois(10000, 7)
```

```{r}
dat.thin <- datathin(dat, family="poisson", epsilon=0.3)
dat.train <- dat.thin$Xtr
dat.test <- dat.thin$Xte
```

We now verify several properties of the data thinning operation.

First, we verify that the expected value of dat.train is $0.3 \times 7$ and the expected value of dat.test is $0.7 \times 7$. 

```{r, hold=TRUE}
mean(dat)
mean(dat.train)
0.3*7
mean(dat.test)
0.7*7
```

We next verify that `dat.train` and `dat.test` are independent, and that they sum to `dat`. We note that throughout this document, we use small empirical correlations as evidence of independence. 

```{r}
all.equal(dat, as.numeric(dat.train+dat.test))
cor(dat.train, dat.test)
```


# Exponential

We next consider exponential random variables. These are also simple, as the distribution only has one parameter. 

```{r}
set.seed(2)
dat <- rexp(100000, rate=1/3)
mean(dat)
```

For this distribution, we demonstrate how to create multiple indendent folds of data using the ``multithin`` function. 

```{r}
folds <- multithin(dat, family="exponential", nfolds=5)
```

Here, `folds` is a list with 5 elements. Each list element stores one fold of data, which in this case means a vector of length 1000.

```{r}
length(folds)
length(folds[[1]])
```

We can verify that, for $m=1,\ldots,5$, the $m$th fold of data is independent of the sum of the remaining folds, and that it has mean $\frac{1}{5} \times 3$, while the remainder has mean $\frac{4}{5} \times 3$.

```{r}
for (m in 1:5) {
  print("-------")
  print(paste("fold", m))
  
  dat.test <- folds[[m]]
  dat.train <- dat - folds[[m]]
  
  print(c(1/5*mean(dat), mean(dat.test)))
  print(c(4/5*mean(dat), mean(dat.train)))

  print(as.numeric(cor(dat.test, dat.train)))
}
```


# Normal distribution

We now show an example of thinning the normal distribution. This is slightly more complicated, as the $N(\mu, \sigma^2)$ distribution has two parameters, and in order to apply data thinning the parameter $\sigma^2$ must be known. 

We start by generating data from a $N(5, 2)$ distribution and applying ``datathin`` with $\epsilon = 0.5$. This time, we let our dataset `dat` be a matrix with dimensions $1000 \times 10$. 

```{r}
dat <- matrix(rnorm(1000*10, mean=5, sd=sqrt(2)), nrow=1000)
```

We first apply data thinning assuming that $\sigma^2=2$ is known. This is passed in as the ``arg`` parameter. We can see that, after applying data thinning, each column of the training set is independent of each column in the test set (once again, we are using small empirical correlations as evidence of independence).

```{r}
res <- datathin(dat, family="normal", epsilon=0.5, arg=2)
dat.train <- res$Xtr
dat.test <- res$Xte
for (j in 1:ncol(dat.train)) {
  print(cor(dat.train[,j], dat.test[,j]))
}
```

The parameter `arg` must either be a scalar (in which case it is assumed that all elements of the data matrix have the same value of $\sigma^2$), or its dimensions must match that of the data.

We now explore the impact of using the "wrong" value of $\sigma^2$ while splitting. We generate a dataset with $10,000$ rows and 

```{r}
```


# Negative binomial

We generate data from a negative binomial distribution. We use the mean and overdispersion parameterization of the negative binomial distribution. 

```{r}
dat <- rnbinom(10000, size=7, mu = 6)
res <- datathin(dat, family="negative binomial", epsilon = 0.2, arg=7)
dat.train <- res$Xtr
dat.test <- res$Xte
0.2*6
mean(dat.train)
0.8*6
mean(dat.test)
as.numeric(cor(dat.train, dat.test))
```

# Gamma 

We verify the same properties as above, assuming that the shape parameter is known. 

```{r}
dat <- rgamma(10000, shape=12, rate=2)
mean(dat)
res <- datathin(dat, family="gamma", epsilon = 0.5, arg=12)
dat.train <- res$Xtr
dat.test <- res$Xte
mean(dat.train)
mean(dat.test)
as.numeric(cor(dat.train, dat.test))
```



# Binomial 

We verify the same properties as above, assuming that the size parameter is known. Note that, if we want to use $\epsilon=0.5$, the size parameter of the binomial distribution must be an even number. 

```{r}
dat <- rbinom(10000, 16, 0.25)
mean(dat)
res <- datathin(dat, family="binomial", epsilon = 0.5, arg=16)
dat.train <- res$Xtr
dat.test <- res$Xte
mean(dat.train)
mean(dat.test)
as.numeric(cor(dat.train, dat.test))
```