---
title: "Introductory tutorial" 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introductory tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE
)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

Welcome to the `datathin` package. In this tutorial, we will show how random variables from different distributions can be split into independent training and test components. For more details, see our pre-print. For information on how to use `datathin` for tasks such as model evaluation or inference after model selection, see our forthcoming tutorials. 

To get started, ensure that you have downloaded the package. Make sure that ``remotes`` is installed by running ``install.packages("remotes")``, then type

```{r,eval=FALSE}
remotes::install_github("anna-neufeld/datathin")
```

```{r}
library(datathin)
library(mvtnorm)
library(extraDistr)
library(ggplot2)
library(patchwork)
```


# Poisson

The Poisson distribution is simple, because there is only one parameter. 

We first generate a vector of 10,000 $Poisson(7)$ random variables. 

```{r}
set.seed(1)
dat <- rpois(10000, 7)
```

Next, we thin the data with the ``datathin`` function. The output of ``datathin`` is an array where the last dimension indexes the folds of the thinned data.

```{r}
dat.thin <- datathin(dat, family="poisson", epsilon=c(0.3, 0.7))
dat.train <- dat.thin[,,1]
dat.test <- dat.thin[,,2]
```

We now verify several properties of the data thinning operation.

First, we verify that the expected value of dat.train is $0.3 \times 7$ and the expected value of dat.test is $0.7 \times 7$. 

```{r, hold=TRUE}
mean(dat)
mean(dat.train)
0.3*7
mean(dat.test)
0.7*7
```

We next verify that `dat.train` and `dat.test` are independent, and that they sum to `dat`. Throughout this document, we use small empirical correlations as evidence of independence. 

```{r}
all.equal(dat, as.numeric(dat.train+dat.test))
cor(dat.train, dat.test)
```


# Exponential

We next consider the exponential distribution. 

```{r}
set.seed(2)
dat <- rexp(100000, rate=1/5)
mean(dat)
```

For this distribution, we will create 5 independent, symmetric folds of the ``K`` argument in the ``datathin`` function.

```{r}
folds <- datathin(dat, family="exponential", K=5)
```

Here, `folds` is a $100,000 \times 1 \times 5$ array. 

```{r}
dim(folds)[3]
length(folds[,,3])
```

We can verify that, for $m=1,\ldots,5$, the $m$th fold of data is independent of the sum of the remaining folds, and that it has mean $\frac{1}{5} \times 5 = 1$, while the remainder has mean $\frac{4}{5} \times 5 = 4$. Once again, we use small empirical correlations as evidence of independence.  

```{r}
for (m in 1:5) {
  print("-------")
  print(paste("fold", m))
  
  dat.test <- folds[,,m]
  dat.train <- dat - folds[,,m]
  
  print(paste("Test set mean", round(mean(dat.test), 3)))
  print(paste("Training set mean:", round(mean(dat.train), 3)))
  print(paste("Sample correlation:", round(as.numeric(cor(dat.test, dat.train)),4)))
}
```

# Gamma 

The results for the exponential family can be extended to any gamma distribution where the shape parameter is known. To thin the gamma distribution we will provide the shape to the ``datathin`` function using the ``arg`` argument. Note that by omitting the ``epsilon`` parameter, we are requesting symmetric training and test sets.

```{r}
dat <- rgamma(10000, shape=12, rate=2)
mean(dat)
res <- datathin(dat, family="gamma", arg=12)
dat.train <- res[,,1]
dat.test <- res[,,2]
mean(dat.train)
mean(dat.test)
as.numeric(cor(dat.train, dat.test))
```

# Normal distribution

We now show an example of thinning the normal distribution. This is slightly more complicated, as the $N(\mu, \sigma^2)$ distribution has two parameters. We will demonstrate data thinning strategies when either the mean $\mu$ or the variance $\sigma^2$ are unknown, and the other is known.

We start by generating data from a $N(5, 2)$ distribution. This time, we let our dataset `dat` be a matrix with dimensions $10000 \times 10$. 

```{r}
set.seed(3)
dat <- matrix(rnorm(10000*10, mean=5, sd=sqrt(2)), nrow=10000)
```

## Thinning the normal distribution with unknown mean

We first apply data thinning assuming that $\sigma^2=2$ is known. This is passed in as the ``arg`` parameter. We can see that, after applying data thinning, each column of the training set is independent of each column in the test set (once again, we are using small empirical correlations as evidence of independence).

```{r}
res <- datathin(dat, family="normal", arg=2)
dat.train <- res[,,1]
dat.test <- res[,,2]

cors <- sapply(1:ncol(dat.train), function(u) round(cor(dat.train[,u], dat.test[,u]), 4))
print(paste("Correlation between train and test in column", 1:10, ":", cors))
```

The parameter `arg` must either be a scalar (in which case it is assumed that all elements of the data matrix have the same value of $\sigma^2$), or its dimensions must match that of the data.

We now explore the impact of using the "wrong" value of $\sigma^2$ while splitting. We generate a dataset with $100,000$ rows and $3$ columns. In the first column, the data are from a $N(5, 0.1)$ distribution. In the second column, they are from a $N(5,2)$ distribution. In the final column, they are from a $N(5,20)$ distribution. 

```{r}
dat <- cbind(rnorm(100000, mean=5, sd=sqrt(0.1)),
             rnorm(100000, mean=5, sd=sqrt(2)),
             rnorm(100000, mean=5, sd=sqrt(20)))
```

First, we datathin but we incorrectly assume that all items in the matrix have $\sigma^2=2$. We see that we have negative correlation between the training and test sets in the first column (where we used a value of $\sigma^2$ that was too big), no correlation in the second column (where we used the correct value of $\sigma^2$), and positive correlation in the last column (where we used a value of $\sigma^2$ that was too small). See our [preprint](https://arxiv.org/abs/2301.07276) for further explanation of this property. 

```{r}
res <- datathin(dat, family="normal", arg=2)
dat.train <- res[,,1]
dat.test <- res[,,2]
cors <- sapply(1:ncol(dat.train), function(u) round(cor(dat.train[,u], dat.test[,u]), 4))
print(paste("Correlation between train and test in column", 1:3, ":", cors))
```

To remedy the situation, we must let `arg` be a matrix that stores the correct $\sigma^2$ values.

```{r}
good_args <- cbind(rep(0.1, 100000), rep(2, 100000), rep(20,100000))
res <- datathin(dat, family="normal", arg=good_args)
dat.train <- res[,,1]
dat.test <- res[,,2]
cors <- sapply(1:ncol(dat.train), function(u) round(as.numeric(cor(dat.train[,u], dat.test[,u])),4))
print(paste("Correlation between train and test in column", 1:3, ":", cors))
```

We quickly note that an error will be thrown if `arg` is not a scalar and its dimensions do not match those of the data. The following code results in an error:

```{r, eval=F}
res <- datathin(dat, family="normal", arg=c(0.1,2,20))
```

## Thinning the normal distribution with unknown variance

We can also thin the normal distribution with unknown variance by recognizing that for $X\sim N(\mu,\sigma^2)$, $(X-\mu)^2 \sim \sigma^2 \chi_1^2 = \text{Gamma}\left(\frac{1}{2},\frac{1}{2\sigma^2}\right)$. In other words, we can convert our normal data with knonw mean into gamma data with known shape, thus allowing us to apply gamma data thinning. This is an example of indirect thinning; see our second [preprint](https://arxiv.org/abs/2303.12931) for more details.

```{r}
dat <- matrix(rnorm(10000*10, mean=5, sd=sqrt(2)), nrow=10000)
var(as.vector(dat))
res <- datathin(dat, family="normal-variance", arg=5)
dat.train <- res[,,1]
dat.test <- res[,,2]
mean(dat.train)
mean(dat.test)
as.numeric(cor(as.vector(dat.train), as.vector(dat.test)))
```

# Negative binomial

We generate data from a negative binomial distribution. We use the mean and overdispersion parameterization of the negative binomial distribution. 

```{r}
dat <- rnbinom(100000, size=7, mu = 6)
res <- datathin(dat, family="negative binomial", epsilon = c(0.2,0.8), arg=7)
dat.train <- res[,,1]
dat.test <- res[,,2]
0.2*6
mean(dat.train)
0.8*6
mean(dat.test)
as.numeric(cor(dat.train, dat.test))
```


# Binomial 

We verify the same properties as above, assuming that the size parameter is known. Note that, if we want to use $\epsilon=0.5$, the size parameter of the binomial distribution must be an even number. 

```{r}
dat <- rbinom(10000, 16, 0.25)
mean(dat)
res <- datathin(dat, family="binomial", arg=16)
dat.train <- res[,,1]
dat.test <- res[,,2]
mean(dat.train)
mean(dat.test)
as.numeric(cor(dat.train, dat.test))
```

# Additional gamma families

The gamma distributions is extremely flexible, and shows up in many thinning recipes. Indeed, we saw this above when we thinned the normal distribution with unknown variance using the gamma distribution. Here we showcase similar strategies that also use the gamma distribution in some way; see our second [preprint](https://arxiv.org/abs/2303.12931) for further details.

## Chi-squared

The chi-squared distribution is a special case of the gamma distribution, where the shape is equal to half of the degrees of freedom. Using this relationship, we can thin a scaled chi-squared distribution with $K$ degrees of freedom into $K$ normal random variables.

```{r}
dat <- 3*rchisq(10000, df=5)
mean(dat)/5
res <- datathin(dat, family="chi-squared", K=5)
dat1 <- res[,,1]
dat2 <- res[,,2]
var(dat1)
var(dat2)
as.numeric(cor(dat1, dat2))
```

## Gamma-Weibull

Similarly, we can thin the gamma distribution into the Weibull distribution.

```{r}
dat <- rgamma(10000, shape=4, rate=4)
res <- datathin(dat, family="gamma-weibull", K=4, arg=3)
dat1 <- res[,,1]
dat2 <- res[,,2]
print(paste("Expected mean:", (4^(-1/3))*gamma(1+1/3)))
mean(dat1)
mean(dat2)
as.numeric(cor(dat1, dat2))
```

## Weibull

We can also thin the Weibull distribution with known shape into gamma random variables.

```{r}
dat <- rweibull(100000, 5, 2)
res <- datathin(dat, family="weibull", K=2, arg=5)
dat1 <- res[,,1]
dat2 <- res[,,2]
print(paste("Expected mean:", (1/2)*(2^(5))))
mean(dat1)
mean(dat2)
as.numeric(cor(dat1, dat2))
```

## Pareto 

Similarly, we can thin the Pareto distribution with known scale into gamma random variables.

```{r}
dat <- rpareto(100000, 6, 2)
res <- datathin(dat, family="pareto", K=2, arg=2)
dat1 <- res[,,1]
dat2 <- res[,,2]
print(paste("Expected mean:", (1/2)/6))
mean(dat1)
mean(dat2)
as.numeric(cor(dat1, dat2))
```

# Multivariate distributions

Data thinning recipes are also available for a subset of multivariate distributions. In both of the following distributions, care must be taken when providing the known parameter.

## Multivariate normal

As the multivariate normal distribution is vector-valued, each row of the $n \times p$ ``data`` matrix corresponds to one observation. To thin this data, we must provide the covariance matrix to the ``datathin`` function. Either ``arg`` can be a single $p \times p$ covariance matrix, or a $n \times p \times p$ array of covariance matrices.

```{r}
rho <- 0.6
Sig <- matrix(0, nrow=4, ncol=4)
for (i in 1:4) {
  for (j in 1:4) {
    Sig[i,j] <- rho^abs(i-j)
  }
}
dat <- rmvnorm(100000, c(1,2,3,4), Sig)
colMeans(dat)
res <- datathin(dat, family="mvnormal", arg=Sig)
dat.train <- res[,,1]
dat.test <- res[,,2]
colMeans(dat.train)
colMeans(dat.test)
max(abs(cor(dat.train, dat.test)))
```

## Multinomial

For the multinomial distribution, each row of the $n\times p$ data matrix represents a realization from the multinomial distribution. The input to ``arg`` is then either a scalar, or a vector of length $n$. Similar to the binomial distribution, the symmetric folds require an even number of trials.

```{r}
dat <- t(rmultinom(100000, 20, c(0.1,0.2,0.3,0.4)))
colMeans(dat)
res <- datathin(dat, family="multinomial", arg=20)
dat.train <- res[,,1]
dat.test <- res[,,2]
colMeans(dat.train)
colMeans(dat.test)
max(abs(cor(dat.train, dat.test)))
```

# Shift and scale families

Outside of the exponential family, certain shift and scale families can also be thinned. 

## Scaled beta

When $X\sim \theta \text{Beta}(\alpha,1)$, $X$ can be thinned so long as $\alpha$ is known. If $\alpha=1$, then this reduces to the simpler setting of thinning $X\sim\text{Uniform}(0,\theta)$.

```{r}
dat <- 10*rbeta(100000, 7, 1)
res <- datathin(dat, family="scaled-beta", K=2, arg=7)
dat1 <- res[,,1]
dat2 <- res[,,2]
print(paste("Expected mean:", 10*(7/2)/(7/2 + 1)))
mean(dat1)
mean(dat2)
as.numeric(cor(dat1, dat2))
```

## Shifted exponential

We can also thin a shifted exponential, assuming that the rate is known.

```{r}
dat <- 6 + rexp(100000, 2)
res <- datathin(dat, family="shifted-exponential", K=2, arg=2)
dat1 <- res[,,1]
dat2 <- res[,,2]
print(paste("Expected mean:", 6 + 1))
mean(dat1)
mean(dat2)
as.numeric(cor(dat1, dat2))
```





