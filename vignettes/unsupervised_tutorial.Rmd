---
title: "Unsupervised learning tutorial" 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Unsupervised tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE
)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

Welcome to the `datathin` package. In this tutorial, we will show how to use data thinning for model evaluation in the context of unsupervised learning. For more information, see our preprint (link coming soon).

Before getting started with this tutorial, we recommend that you read our introductory tutorial. You also should ensure that the package is installed and loaded. 


```{r,eval=FALSE}
remotes::install_github("anna-neufeld/datathin")
```

```{r}
library(datathin)
library(ggplot2)
```


# Data with one true cluster

We generate $X_{ij} \sim N(0,1)$ for $i=1,\ldots, 150$ and $j=1,2$. This data has just one true cluster, since all observations come from the same distribution. The code below generates the data and plots the data.

```{r}
set.seed(1)
n <- 150
p <- 2
X<- matrix(rnorm(n*p, mean=0, sd=1), ncol=p)

ggplot(data=NULL, aes(x=X[,1], y=X[,2]))+geom_point()+
  theme_bw()+coord_fixed()+ggtitle("All data")
```

Suppose that we didn't know that the data did not have any clusters, and we wanted to compare models with 1,2,3, etc. clusters to see which one provided the best fit to the data. Here, we will define "best fit" using the within-cluster mean squared error (MSE). 

## The naive method

As a first step, we might try using the "naive" method. This involves estimating the clusters and computing the within-cluster MSE on the same data. We first write the following function, which is designed to return the within-cluster MSE.

```{r}
cluster.mse.naive <- function(dat, clusterlabs) {
  totSS <- 0
  for (lab in unique(clusterlabs)) {
    clustdat <- dat[clusterlabs==lab,, drop='F']
    meanvec <- colMeans(dat[clusterlabs==lab,, drop='F'])
    ss <- apply(clustdat, 1, function(u) sum((u-meanvec)^2))
    totSS <- totSS+sum(ss)
  }
  n <- NROW(dat)
  p <- NCOL(dat)
  return(totSS/(n*p))
}
```


We now compare the within-cluster MSE of a model with 1 cluster to the within-cluster MSE of a model with 3 clusters, using our dataset above. 

```{r}
one.cluster <- as.factor(rep(1,n))
three.clusters <- as.factor(kmeans(X, centers=3)$cluster)
mse.1 <- cluster.mse.naive(X, one.cluster)
mse.3 <- cluster.mse.naive(X, three.clusters)

library(patchwork)
p1 <- ggplot(data=NULL, aes(x=X[,1], y=X[,2], col=one.cluster))+geom_point()+
  theme_bw()+coord_fixed()+ggtitle("All data, 1 cluster", round(mse.1,3))
library(patchwork)
p3 <- ggplot(data=NULL, aes(x=X[,1], y=X[,2], col=three.clusters))+geom_point()+
  theme_bw()+coord_fixed()+ggtitle("All data, 3 clusters", round(mse.3,3))
p1+p3
```

Even though this data only has one true cluster, when we use this "naive" method, the model with three clusters has a lower within-cluster MSE due to overfitting. In fact, as shown in the plot below, the within-cluster MSE is monotone decreasing in the number of clusters that we estimate. 
This motivates the need for data thinning.

```{r}
clusters.full <- sapply(1:10, function(u) kmeans(X, centers= u)$cluster)
results.naive <-  apply(clusters.full, 2, function(u) cluster.mse.naive(X, u))
ggplot(data = NULL)+
  geom_line(aes(x=1:10, y=results.naive, col="Naive method"), lwd=1.5)+
  theme_bw()+
  theme(axis.text = element_text(size=16), axis.title = element_text(size=18))+
  scale_x_continuous(breaks=seq(0,10,by=2))+
  xlab("Number of Clusters") + ylab("Total within-cluster MSE")+labs(col="")
```


We quickly note that sample splitting is not an option here.

## Data thinning

Using the same dataset as above, we now apply data thinning. We call the function `datathin` with `family="normal"`. For simplicity, we assume that the noise variance $\sigma^2$ is known. 

```{r}
X.thin <- datathin(X, family="normal", epsilon=0.5, arg=1)
Xtrain <- X.thin$Xtr
Xtest <- X.thin$Xte
p1 <- ggplot(data=NULL, aes(x=X[,1], y=X[,2]))+geom_point()+
  theme_bw()+xlim(c(-3,3))+ylim(c(-3,3))+
  coord_fixed()+ggtitle("All data")
p2 <- ggplot(data=NULL, aes(x=Xtrain[,1], y=Xtrain[,2]))+geom_point()+
  theme_bw()+xlim(c(-3,3))+ylim(c(-3,3))+
  coord_fixed()+ggtitle("Training set")
p3 <- ggplot(data=NULL, aes(x=Xtest[,1], y=Xtest[,2]))+geom_point()+
  theme_bw()+xlim(c(-3,3))+ylim(c(-3,3))+
  coord_fixed()+ggtitle("Test set")
p1+p2+p3
```
Now, if we cluster the training set and apply those labels to the test set, we do not get overfitting. 

```{r}
cluster.train <- as.factor(kmeans(Xtrain, centers=3)$cluster)
p2 <- ggplot(data=NULL, aes(x=Xtrain[,1], y=Xtrain[,2], col=cluster.train))+geom_point()+
  theme_bw()+xlim(c(-3,3))+ylim(c(-3,3))+
  coord_fixed()+ggtitle("Training set")
p3 <- ggplot(data=NULL, aes(x=Xtest[,1], y=Xtest[,2], col=cluster.train))+geom_point()+
  theme_bw()+xlim(c(-3,3))+ylim(c(-3,3))+
  coord_fixed()+ggtitle("Test set")
p2+p3+plot_layout(guides="collect")
```

This helps us make an MSE curve that is *not* monotonically decreasing in the number of clusters, and that correctly identifies 1 cluster as the best-fit for this data. We first need to write a data thinning MSE function. 

```{r}
cluster.mse.datathin <- function(dat.train, dat.test, clusterlabs) {
  totSS <- 0
  for (lab in unique(clusterlabs)) {
    clustdat.test <- dat.test[clusterlabs==lab,, drop='F']
    meanvec <- colMeans(dat.train[clusterlabs==lab,, drop='F'])
    ss <- apply(clustdat.test, 1, function(u) sum((u-meanvec)^2))
    totSS <- totSS+sum(ss)
  }
  n <- NROW(dat.test)
  p <- NCOL(dat.test)
  return(totSS/(n*p))
}
```

We can now apply it to our data. To put the naive method and data thinning on the same scale in terms of the MSE, we can multiply the data thinning MSE by $1/\epsilon$, where for us $\epsilon = 0.5$. 

```{r}
clusters.full <- sapply(1:10, function(u) kmeans(X, centers= u)$cluster)
results.naive <-  apply(clusters.full, 2, function(u) cluster.mse.naive(X, u))

clusters.train <- sapply(1:10, function(u) kmeans(Xtrain, centers= u)$cluster)
results.datathin <- apply( clusters.train, 2, function(u) cluster.mse.datathin(Xtrain,Xtest, u))

eps = 0.5

ggplot(data = NULL)+
  geom_line(aes(x=1:10, y=results.naive, col="Naive method"), lwd=1.5)+
  geom_line(aes(x=1:10, y=results.datathin/eps, col="Data thinning"), lwd=1.5)+
  theme_bw()+
  scale_x_continuous(breaks=seq(0,10,by=2))+
  xlab("Number of Clusters") + ylab("Total within-cluster MSE")+labs(col="")
```

We see that this new method correctly identified that there is only one true cluster in this data, since the within-cluster MSE is minimized with 1 cluster.


# Data with three true clusters

We now study the performance of the naive method and of data thinning when there truly are clusters in our dataset. 

To start, we generate a new dataset. We lstill let $n=150$ and $p=2$. 

```{r}
p <- 2
n <- 150
trueClusters <- as.factor(rep(c(1,2,3), each=n/3))
X <- rbind(
  matrix(rnorm(n/3*p, -4,1), ncol=p),
  matrix(rnorm(n/3*p, 0,1), ncol=p),
  matrix(rnorm(n/3*p, 4,1), ncol=p)
)
ggplot(data=NULL, aes(x=X[,1], y=X[,2], col=trueClusters))+geom_point()
```

We first visualize what happens to this data under data thinning.

```{r}
X.thin <- datathin(X, family="normal", epsilon=0.5, arg=1)
Xtrain <- X.thin$Xtr
Xtest <- X.thin$Xte
p1 <- ggplot(data=NULL, aes(x=X[,1], y=X[,2], col=trueClusters))+geom_point()+
  theme_bw()+xlim(c(-6,6))+ylim(c(-6,6))+
  coord_fixed()+ggtitle("All data")
p2 <- ggplot(data=NULL, aes(x=Xtrain[,1], y=Xtrain[,2], col=trueClusters))+geom_point()+
  theme_bw()+xlim(c(-6,6))+ylim(c(-6,6))+
  coord_fixed()+ggtitle("Training set")
p3 <- ggplot(data=NULL, aes(x=Xtest[,1], y=Xtest[,2], col=trueClusters))+geom_point()+
  theme_bw()+xlim(c(-6,6))+ylim(c(-6,6))+
  coord_fixed()+ggtitle("Test set")
p1+p2+p3+plot_layout(guides="collect")
```

We see that data thinning preserves the true clusters, since it simply scales the mean of the observations by a known constant. 

We now compare the within-cluster MSE curves of the naive method and data thinning. 

```{r}
clusters.full <- sapply(1:10, function(u) kmeans(X, centers= u)$cluster)
results.naive <-  apply(clusters.full, 2, function(u) cluster.mse.naive(X, u))

clusters.train <- sapply(1:10, function(u) kmeans(Xtrain, centers= u)$cluster)
results.datathin <- apply( clusters.train, 2, function(u) cluster.mse.datathin(Xtrain,Xtest, u))

eps = 0.5

ggplot(data = NULL)+
  geom_line(aes(x=1:10, y=results.naive, col="Naive method"), lwd=1.5)+
  geom_line(aes(x=1:10, y=results.datathin/eps, col="Data thinning"), lwd=1.5)+
  theme_bw()+
  theme(axis.text = element_text(size=16), axis.title = element_text(size=18))+
  scale_x_continuous(breaks=seq(0,10,by=2))+
  xlab("Number of Clusters") + ylab("Total within-cluster MSE")

```

We see that data thinning minimizes the MSE at $3$, which is the true number of clusters. Thus, data thinning priovides a principled way to select the true number of clusters. With the naive method, we must resort to looking for an ``elbow" in this MSE plot, which is much more heuristic. 

# Alternate choices of epsilon

Our function for computing the within-cluster MSE of data thinning must be updated if the parameter $\epsilon$ used in data thinning is not $0.5$. We will give an example of this in an upcoming tutorials.  



# Data thinning with multiple folds.

In an upcoming tutorial, we will show how to perform cross-validation using multiple folds


# Alternate distributions

The code in this tutorial can be seamlessly updated to handle data that comes from a different distribution. We simply need to update the ``family`` argument in our calls to the ``datathin`` function. Please see our introductory tutorial for a comprehensive list of the distributions that are available. 

# Alternate unsupervised learning models

Instead of estimating the number of clusters, we might wish to perform an analysis where we estimate the rank of a matrix (e.g. estimate the number of principal components that should be retained for further analysis). We will give an example of the principal components task in a future tutorial.